# Attention Is All You Need


## RESEARCH PAPER LINKS
| Paper | Topic |
| ----- | -----|
| 1. [**Attention Is All You Need**](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) | Introduction to Transformer |
| 2. [**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**](https://arxiv.org/pdf/1810.04805.pdf) | BERT 2018 paper |
| 3. [**Improving Language Understanding by Generative Pre-Training**](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | GPT 1 |

------------------------------

## ILLUSTRATIVE ACTICLES

| Paper |
| ----- | 
| 1. [**Transformer**]( http://jalammar.github.io/illustrated-transformer/)
| 2. [**Bidirectional Encoder Representations from Transformers**](http://jalammar.github.io/illustrated-bert/) 
